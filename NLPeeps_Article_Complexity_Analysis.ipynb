{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pk8MIFad0VT",
        "outputId": "22c109b7-5e7d-433b-ad19-8a7841470510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cmudict (from textstat)\n",
            "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (8.6.1)\n",
            "Requirement already satisfied: importlib-resources>=5 in /usr/local/lib/python3.11/dist-packages (from cmudict->textstat) (6.5.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=5->cmudict->textstat) (3.21.0)\n",
            "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, cmudict, textstat\n",
            "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat\n",
        "\n",
        "import textstat\n",
        "import pandas as pd\n",
        "\n",
        "# Function to compute readability metrics\n",
        "def calculate_readability_metrics(text):\n",
        "    return {\n",
        "        \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(text),\n",
        "        \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(text),\n",
        "        \"Gunning Fog Score\": textstat.gunning_fog(text),\n",
        "        \"SMOG Index\": textstat.smog_index(text)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bHYNf2jegASV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ABC\n",
        "# Load CSV file\n",
        "file_path = '/content/dyslexia_friendly_texts_all_20.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract column names (LLM names)\n",
        "llm_names = df.columns[1:]  # Exclude the first column (original text)\n",
        "\n",
        "# Prepare results storage\n",
        "results = []\n",
        "\n",
        "# Process each row (publication)\n",
        "for index, row in df.iterrows():\n",
        "    original_text = row.iloc[0]  # First column: Original unsimplified text\n",
        "\n",
        "    # Compute metrics for original text\n",
        "    original_metrics = calculate_readability_metrics(original_text)\n",
        "    results.append({\n",
        "        \"Publication\": index + 1,\n",
        "        \"LLM\": \"Original\",\n",
        "        **original_metrics\n",
        "    })\n",
        "\n",
        "    # Compute metrics for each LLM's simplification\n",
        "    for llm in llm_names:\n",
        "        simplified_text = row[llm]\n",
        "        simplified_metrics = calculate_readability_metrics(simplified_text)\n",
        "\n",
        "        results.append({\n",
        "            \"Publication\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **simplified_metrics\n",
        "        })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(df_results)\n",
        "# Display results in Jupyter Notebook\n",
        "#tools.display_dataframe_to_user(name=\"Readability Metrics\", dataframe=df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "collapsed": true,
        "id": "INYG-LPbeqE6",
        "outputId": "c044e7a3-c4ee-4282-bc12-ad8c71397ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Publication         LLM  Flesch-Kincaid Reading Ease  \\\n",
            "0             1    Original                        50.16   \n",
            "1             1      GPT-4o                        69.07   \n",
            "2             1     Llama 3                        58.58   \n",
            "3             1  Gemini 2.0                        79.26   \n",
            "4             2    Original                        34.26   \n",
            "..          ...         ...                          ...   \n",
            "75           19  Gemini 2.0                        88.02   \n",
            "76           20    Original                        51.04   \n",
            "77           20      GPT-4o                        51.04   \n",
            "78           20     Llama 3                        75.20   \n",
            "79           20  Gemini 2.0                        77.94   \n",
            "\n",
            "    Flesch-Kincaid Grade Level  Gunning Fog Score  SMOG Index  \n",
            "0                         11.5              13.64        13.4  \n",
            "1                          6.3               9.28        10.5  \n",
            "2                          8.2               9.29        11.2  \n",
            "3                          4.4               5.08         8.8  \n",
            "4                         13.5              16.00        15.9  \n",
            "..                         ...                ...         ...  \n",
            "75                         3.1               4.86         7.6  \n",
            "76                         9.1              11.50        11.6  \n",
            "77                         9.1              11.50        11.6  \n",
            "78                         6.0               8.28         9.5  \n",
            "79                         4.9               6.45         8.2  \n",
            "\n",
            "[80 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gvQ63O-oFKv",
        "outputId": "fcaf92d8-801d-48d1-b0ba-82fb787c0c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-4.0.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading sentence_transformers-4.0.2-py3-none-any.whl (340 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m340.6/340.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 3.4.1\n",
            "    Uninstalling sentence-transformers-3.4.1:\n",
            "      Successfully uninstalled sentence-transformers-3.4.1\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import textstat\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/dyslexia_friendly_texts_all_20.csv\"\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Extract LLM names\n",
        "llm_names = df.columns[1:]\n",
        "\n",
        "# Load sentence embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def semantic_similarity(text1, text2):\n",
        "    text1, text2 = str(text1), str(text2)\n",
        "    if not text1.strip() or not text2.strip(): return 0.0\n",
        "    emb1, emb2 = model.encode(text1), model.encode(text2)\n",
        "    if np.all(emb1==0) or np.all(emb2==0): return 0.0\n",
        "    sim = 1 - cosine(emb1, emb2)\n",
        "    return sim if not np.isnan(sim) else 1.0 if np.array_equal(emb1, emb2) else 0.0\n",
        "\n",
        "def compression_penalty(original, simplified):\n",
        "    orig_len = len(str(original).split())\n",
        "    simp_len = len(str(simplified).split())\n",
        "    if orig_len == 0: return 1.0\n",
        "    return max(0, 1 - (simp_len / orig_len))\n",
        "\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    sim = semantic_similarity(original, simplified)\n",
        "    penalty = compression_penalty(original, simplified)\n",
        "    return sim * (1 - penalty)\n",
        "\n",
        "# --- Define metric types and weights ---\n",
        "invert_cols = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"Coleman-Liau Index\"]\n",
        "retain_cols = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]\n",
        "all_metrics = retain_cols + invert_cols\n",
        "\n",
        "# Custom weights - adjust as needed\n",
        "#weights_dict = {\n",
        "#    \"Flesch-Kincaid Reading Ease\": 0.20,\n",
        "#    \"BMPS\": 0.40,\n",
        "#    \"Flesch-Kincaid Grade Level\": 0.15,\n",
        "#    \"Gunning Fog Score\": 0.15,\n",
        "#    \"Coleman-Liau Index\": 0.10\n",
        "#}\n",
        "#weights_array = np.array([weights_dict[col] for col in all_metrics])\n",
        "\n",
        "# --- Define metric types and weights ---\n",
        "invert_cols = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"Coleman-Liau Index\"]\n",
        "retain_cols = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]\n",
        "all_metrics = retain_cols + invert_cols\n",
        "\n",
        "# Equal weights for all metrics\n",
        "num_metrics = len(all_metrics)\n",
        "weights_array = np.ones(num_metrics) / num_metrics  # Equal weight for each metric (1/n)\n",
        "\n",
        "print(f\"Using equal weights ({1/num_metrics:.2f}) for all {num_metrics} metrics\")\n",
        "\n",
        "# --- Collect raw metrics ---\n",
        "print(\"Collecting raw metrics...\")\n",
        "records = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    original = str(row[\"Article\"])\n",
        "    if not original.strip():\n",
        "        print(f\"Warning: Skipping row {index+1} due to empty 'Article'.\")\n",
        "        continue\n",
        "\n",
        "    for llm in llm_names:\n",
        "        simplified = str(row[llm])\n",
        "        if not simplified.strip():\n",
        "            metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": 0,\n",
        "                \"Flesch-Kincaid Grade Level\": 20,\n",
        "                \"Gunning Fog Score\": 20,\n",
        "                \"Coleman-Liau Index\": 20,\n",
        "                \"BMPS\": 0.0\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified),\n",
        "                \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified),\n",
        "                \"Gunning Fog Score\": textstat.gunning_fog(simplified),\n",
        "                \"Coleman-Liau Index\": textstat.coleman_liau_index(simplified),\n",
        "                \"BMPS\": balanced_meaning_preservation(original, simplified)\n",
        "            }\n",
        "        records.append({\n",
        "            \"Article ID\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "df_raw = pd.DataFrame(records)\n",
        "print(f\"Collected metrics for {len(df_raw)} Article x LLM combinations.\")\n",
        "\n",
        "# --- APPROACH 1: Global normalization followed by aggregation and TOPSIS ---\n",
        "print(\"\\n=== APPROACH 1: Global Normalization → Aggregation → TOPSIS ===\")\n",
        "\n",
        "# Create a copy for global normalization\n",
        "df_global = df_raw.copy()\n",
        "\n",
        "# Invert metrics where higher is worse\n",
        "for col in invert_cols:\n",
        "    max_val = df_global[col].max()\n",
        "    min_val = df_global[col].min()\n",
        "    df_global[col] = max_val + min_val - df_global[col]\n",
        "\n",
        "# Normalize all metrics globally\n",
        "for col in all_metrics:\n",
        "    min_val = df_global[col].min()\n",
        "    max_val = df_global[col].max()\n",
        "    if max_val > min_val:\n",
        "        df_global[col] = (df_global[col] - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        df_global[col] = 0.5  # Handle constant values\n",
        "\n",
        "# Aggregate metrics by LLM\n",
        "df_agg_global = df_global.groupby(\"LLM\")[all_metrics].mean()\n",
        "\n",
        "# Apply TOPSIS on aggregated metrics\n",
        "decision_matrix = df_agg_global[all_metrics].to_numpy()\n",
        "ideal_best = np.max(decision_matrix, axis=0)\n",
        "ideal_worst = np.min(decision_matrix, axis=0)\n",
        "\n",
        "D_plus = np.sqrt(np.sum(weights_array * (decision_matrix - ideal_best)**2, axis=1))\n",
        "D_minus = np.sqrt(np.sum(weights_array * (decision_matrix - ideal_worst)**2, axis=1))\n",
        "\n",
        "topsis_global = D_minus / (D_plus + D_minus)\n",
        "df_agg_global[\"TOPSIS_Global\"] = topsis_global\n",
        "df_agg_global[\"Global_Rank\"] = df_agg_global[\"TOPSIS_Global\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# --- APPROACH 2: Per-article normalization and TOPSIS ---\n",
        "print(\"\\n=== APPROACH 2: Per-Article Normalization → Per-Article TOPSIS → Aggregation ===\")\n",
        "\n",
        "article_scores = []\n",
        "\n",
        "for article_id, group in df_raw.groupby(\"Article ID\"):\n",
        "    df_article = group.copy()\n",
        "\n",
        "    # Invert where higher is worse for this article\n",
        "    for col in invert_cols:\n",
        "        max_val = df_article[col].max()\n",
        "        min_val = df_article[col].min()\n",
        "        df_article[col] = max_val + min_val - df_article[col]\n",
        "\n",
        "    # Normalize within this article\n",
        "    for col in all_metrics:\n",
        "        min_val = df_article[col].min()\n",
        "        max_val = df_article[col].max()\n",
        "        if max_val > min_val:\n",
        "            df_article[col] = (df_article[col] - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            df_article[col] = 0.5  # Handle constant values\n",
        "\n",
        "    # TOPSIS for this article\n",
        "    M = df_article[all_metrics].to_numpy()\n",
        "    ideal_best = np.max(M, axis=0)\n",
        "    ideal_worst = np.min(M, axis=0)\n",
        "\n",
        "    D_pos = np.sqrt(np.sum(weights_array * (M - ideal_best)**2, axis=1))\n",
        "    D_neg = np.sqrt(np.sum(weights_array * (M - ideal_worst)**2, axis=1))\n",
        "\n",
        "    topsis_scores = D_neg / (D_pos + D_neg)\n",
        "\n",
        "    df_article[\"TOPSIS_Local\"] = topsis_scores\n",
        "    df_article[\"Local_Rank\"] = df_article[\"TOPSIS_Local\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "    article_scores.append(df_article)\n",
        "\n",
        "df_all_local = pd.concat(article_scores)\n",
        "\n",
        "# Aggregate local TOPSIS scores\n",
        "df_agg_local = df_all_local.groupby(\"LLM\").agg({\n",
        "    \"TOPSIS_Local\": [\"mean\", \"std\", \"min\", \"max\"],\n",
        "    \"Local_Rank\": [\"mean\", \"std\", \"min\", \"max\"]\n",
        "})\n",
        "\n",
        "# Flatten column names\n",
        "df_agg_local.columns = ['_'.join(col).strip() for col in df_agg_local.columns.values]\n",
        "df_agg_local[\"Local_Rank_Agg\"] = df_agg_local[\"TOPSIS_Local_mean\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# --- COMBINED APPROACH: Hybrid scoring and comprehensive ranking ---\n",
        "print(\"\\n=== COMBINED APPROACH: Hybrid Evaluation ===\")\n",
        "\n",
        "# Join the results from both approaches\n",
        "df_combined = pd.merge(\n",
        "    df_agg_global[[\"TOPSIS_Global\", \"Global_Rank\"]],\n",
        "    df_agg_local[[\"TOPSIS_Local_mean\", \"TOPSIS_Local_std\", \"Local_Rank_Agg\"]],\n",
        "    left_index=True, right_index=True\n",
        ")\n",
        "\n",
        "# Calculate a hybrid score (weighted combination of global and local TOPSIS)\n",
        "# Adjust weights (0.5, 0.5) as needed to prioritize global vs local metrics\n",
        "global_weight = 0.5\n",
        "local_weight = 0.5\n",
        "\n",
        "# Normalize the TOPSIS scores to [0,1] before combining\n",
        "g_min, g_max = df_combined[\"TOPSIS_Global\"].min(), df_combined[\"TOPSIS_Global\"].max()\n",
        "l_min, l_max = df_combined[\"TOPSIS_Local_mean\"].min(), df_combined[\"TOPSIS_Local_mean\"].max()\n",
        "\n",
        "if g_max > g_min:\n",
        "    df_combined[\"TOPSIS_Global_Norm\"] = (df_combined[\"TOPSIS_Global\"] - g_min) / (g_max - g_min)\n",
        "else:\n",
        "    df_combined[\"TOPSIS_Global_Norm\"] = 0.5\n",
        "\n",
        "if l_max > l_min:\n",
        "    df_combined[\"TOPSIS_Local_Norm\"] = (df_combined[\"TOPSIS_Local_mean\"] - l_min) / (l_max - l_min)\n",
        "else:\n",
        "    df_combined[\"TOPSIS_Local_Norm\"] = 0.5\n",
        "\n",
        "# Calculate hybrid score and rank\n",
        "df_combined[\"Hybrid_Score\"] = (\n",
        "    global_weight * df_combined[\"TOPSIS_Global_Norm\"] +\n",
        "    local_weight * df_combined[\"TOPSIS_Local_Norm\"]\n",
        ")\n",
        "df_combined[\"Hybrid_Rank\"] = df_combined[\"Hybrid_Score\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# Calculate a consistency-adjusted score\n",
        "# Models with high consistency (low std dev) get a bonus\n",
        "consistency_weight = 0.2  # Adjust as needed (0.0 to ignore consistency)\n",
        "max_std = df_combined[\"TOPSIS_Local_std\"].max()\n",
        "min_std = df_combined[\"TOPSIS_Local_std\"].min()\n",
        "\n",
        "if max_std > min_std:\n",
        "    consistency_factor = 1 - ((df_combined[\"TOPSIS_Local_std\"] - min_std) / (max_std - min_std))\n",
        "else:\n",
        "    consistency_factor = np.ones(len(df_combined))\n",
        "\n",
        "df_combined[\"Consistency_Adjusted_Score\"] = (\n",
        "    df_combined[\"Hybrid_Score\"] * (1 + consistency_weight * consistency_factor)\n",
        ")\n",
        "df_combined[\"Final_Rank\"] = df_combined[\"Consistency_Adjusted_Score\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# Add raw metrics averages for reference\n",
        "df_raw_means = df_raw.groupby(\"LLM\")[all_metrics].mean()\n",
        "df_combined = pd.merge(df_combined, df_raw_means, left_index=True, right_index=True)\n",
        "\n",
        "# Sort and display the final results\n",
        "df_final = df_combined.sort_values(\"Final_Rank\")\n",
        "print(\"\\n=== FINAL COMBINED RANKINGS ===\")\n",
        "print(df_final[[\"Final_Rank\", \"Consistency_Adjusted_Score\", \"Global_Rank\", \"Local_Rank_Agg\",\n",
        "                \"TOPSIS_Global\", \"TOPSIS_Local_mean\", \"TOPSIS_Local_std\"]])\n",
        "\n",
        "# Export to CSV if needed\n",
        "# df_final.to_csv(\"llm_combined_evaluation.csv\")\n",
        "\n",
        "# Return dataframes for further analysis\n",
        "print(\"\\nComplete evaluation finished. Results available in df_final.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GmxTM6eKt18",
        "outputId": "73b65861-3c2c-4c46-c7b7-f5193e456d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using equal weights (0.20) for all 5 metrics\n",
            "Collecting raw metrics...\n",
            "Collected metrics for 60 Article x LLM combinations.\n",
            "\n",
            "=== APPROACH 1: Global Normalization → Aggregation → TOPSIS ===\n",
            "\n",
            "=== APPROACH 2: Per-Article Normalization → Per-Article TOPSIS → Aggregation ===\n",
            "\n",
            "=== COMBINED APPROACH: Hybrid Evaluation ===\n",
            "\n",
            "=== FINAL COMBINED RANKINGS ===\n",
            "            Final_Rank  Consistency_Adjusted_Score  Global_Rank  \\\n",
            "LLM                                                               \n",
            "Gemini 2.0         1.0                    1.200000          1.0   \n",
            "GPT-4o             2.0                    0.215746          2.0   \n",
            "Llama 3            3.0                    0.000000          3.0   \n",
            "\n",
            "            Local_Rank_Agg  TOPSIS_Global  TOPSIS_Local_mean  TOPSIS_Local_std  \n",
            "LLM                                                                             \n",
            "Gemini 2.0             1.0       0.876993           0.736897          0.193925  \n",
            "GPT-4o                 2.0       0.217510           0.412009          0.267576  \n",
            "Llama 3                3.0       0.024751           0.328071          0.221581  \n",
            "\n",
            "Complete evaluation finished. Results available in df_final.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import textstat\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/dyslexia_friendly_texts_all_20.csv\"\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Extract LLM names\n",
        "llm_names = df.columns[1:]\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def semantic_similarity(text1, text2):\n",
        "    text1, text2 = str(text1), str(text2)\n",
        "    if not text1.strip() or not text2.strip(): return 0.0\n",
        "    emb1, emb2 = model.encode(text1), model.encode(text2)\n",
        "    if np.all(emb1 == 0) or np.all(emb2 == 0): return 0.0\n",
        "    sim = 1 - cosine(emb1, emb2)\n",
        "    return sim if not np.isnan(sim) else 1.0 if np.array_equal(emb1, emb2) else 0.0\n",
        "\n",
        "def compression_penalty(original, simplified):\n",
        "    orig_len = len(str(original).split())\n",
        "    simp_len = len(str(simplified).split())\n",
        "    if orig_len == 0: return 1.0\n",
        "    return max(0, 1 - (simp_len / orig_len))\n",
        "\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    sim = semantic_similarity(original, simplified)\n",
        "    penalty = compression_penalty(original, simplified)\n",
        "    return sim * (1 - penalty)\n",
        "\n",
        "# --- Collect Metrics Per Article x LLM ---\n",
        "records = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    original = str(row[\"Article\"])\n",
        "    for llm in llm_names:\n",
        "        simplified = str(row[llm])\n",
        "        if not simplified.strip():\n",
        "            metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": 0,\n",
        "                \"Flesch-Kincaid Grade Level\": 20,\n",
        "                \"Gunning Fog Score\": 20,\n",
        "                \"Coleman-Liau Index\": 20,\n",
        "                \"BMPS\": 0.0\n",
        "            }\n",
        "        else:\n",
        "            metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified),\n",
        "                \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified),\n",
        "                \"Gunning Fog Score\": textstat.gunning_fog(simplified),\n",
        "                \"Coleman-Liau Index\": textstat.coleman_liau_index(simplified),\n",
        "                \"BMPS\": balanced_meaning_preservation(original, simplified)\n",
        "            }\n",
        "        records.append({\n",
        "            \"Article ID\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **metrics\n",
        "        })\n",
        "\n",
        "df_raw = pd.DataFrame(records)\n",
        "\n",
        "# --- Normalize & Apply TOPSIS Per Article ---\n",
        "article_scores = []\n",
        "\n",
        "invert_cols = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"Coleman-Liau Index\"]\n",
        "all_metrics = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"] + invert_cols\n",
        "\n",
        "for article_id, group in df_raw.groupby(\"Article ID\"):\n",
        "    df_article = group.copy()\n",
        "\n",
        "    # Invert where higher is worse\n",
        "    for col in invert_cols:\n",
        "        max_val = df_article[col].max()\n",
        "        min_val = df_article[col].min()\n",
        "        df_article[col] = max_val + min_val - df_article[col]\n",
        "\n",
        "    # Normalize\n",
        "    for col in all_metrics:\n",
        "        min_val = df_article[col].min()\n",
        "        max_val = df_article[col].max()\n",
        "        if max_val != min_val:\n",
        "            df_article[col] = (df_article[col] - min_val) / (max_val - min_val)\n",
        "        else:\n",
        "            df_article[col] = 0.5\n",
        "\n",
        "    # TOPSIS\n",
        "    M = df_article[all_metrics].to_numpy()\n",
        "    weights = np.ones(len(all_metrics)) / len(all_metrics)\n",
        "    ideal_best = np.max(M, axis=0)\n",
        "    ideal_worst = np.min(M, axis=0)\n",
        "    D_pos = np.sqrt(np.sum(weights * (M - ideal_best)**2, axis=1))\n",
        "    D_neg = np.sqrt(np.sum(weights * (M - ideal_worst)**2, axis=1))\n",
        "    topsis_scores = D_neg / (D_pos + D_neg)\n",
        "\n",
        "    df_article[\"TOPSIS\"] = topsis_scores\n",
        "    article_scores.append(df_article[[\"Article ID\", \"LLM\", \"TOPSIS\"] + all_metrics])\n",
        "\n",
        "df_all = pd.concat(article_scores)\n",
        "\n",
        "# --- Aggregate Across Articles ---\n",
        "df_summary = df_all.groupby(\"LLM\").agg({\n",
        "    \"TOPSIS\": [\"mean\", \"std\"],\n",
        "    **{m: \"mean\" for m in all_metrics}\n",
        "})\n",
        "\n",
        "df_summary.columns = ['_'.join(col).strip() for col in df_summary.columns.values]\n",
        "df_summary[\"TOPSIS Rank\"] = df_summary[\"TOPSIS_mean\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# Show final summary\n",
        "print(df_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnxuo25yIfnc",
        "outputId": "ebd1a2c4-62f0-45c0-9728-21be24fe8f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            TOPSIS_mean  TOPSIS_std  Flesch-Kincaid Reading Ease_mean  \\\n",
            "LLM                                                                     \n",
            "GPT-4o         0.412009    0.267576                          0.244011   \n",
            "Gemini 2.0     0.736897    0.193925                          0.895808   \n",
            "Llama 3        0.328071    0.221581                          0.325690   \n",
            "\n",
            "            BMPS_mean  Flesch-Kincaid Grade Level_mean  \\\n",
            "LLM                                                      \n",
            "GPT-4o       0.726039                         0.256018   \n",
            "Gemini 2.0   0.443229                         0.888235   \n",
            "Llama 3      0.296927                         0.310223   \n",
            "\n",
            "            Gunning Fog Score_mean  Coleman-Liau Index_mean  TOPSIS Rank  \n",
            "LLM                                                                       \n",
            "GPT-4o                    0.269753                 0.351486          2.0  \n",
            "Gemini 2.0                0.977404                 0.802065          1.0  \n",
            "Llama 3                   0.222095                 0.304016          3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIw_wjEjIDEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import ace_tools as tools\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import textstat # Make sure textstat is imported\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/dyslexia_friendly_texts_all_20.csv\" # Make sure this path is correct\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "\n",
        "# Handle potential missing values by filling with an empty string\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Extract LLM names (all columns except \"Article\")\n",
        "llm_names = df.columns[1:]\n",
        "\n",
        "# Load sentence embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# --- Helper Functions (Semantic Similarity, Compression, BMPS) ---\n",
        "# (Using the robust versions from the previous iteration)\n",
        "def semantic_similarity(text1, text2):\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "    if not text1 or not text2: return 0.0\n",
        "    emb1 = model.encode(text1)\n",
        "    emb2 = model.encode(text2)\n",
        "    if np.all(emb1==0) or np.all(emb2==0): return 0.0\n",
        "    similarity = 1 - cosine(emb1, emb2)\n",
        "    # Handle potential NaN from cosine if vectors are identical or zero\n",
        "    return similarity if not np.isnan(similarity) else (1.0 if np.array_equal(emb1, emb2) else 0.0)\n",
        "\n",
        "\n",
        "def compression_penalty(original, simplified):\n",
        "    original = str(original)\n",
        "    simplified = str(simplified)\n",
        "    orig_len = len(original.split())\n",
        "    simp_len = len(simplified.split())\n",
        "    if orig_len == 0: return 1.0\n",
        "    # Avoid division by zero if original length is 0 but simplified is not\n",
        "    if orig_len == 0 and simp_len > 0: return 0.0 # Or handle as appropriate\n",
        "    return max(0, 1 - (simp_len / orig_len))\n",
        "\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    original = str(original)\n",
        "    simplified = str(simplified)\n",
        "    if not original or not simplified: return 0.0\n",
        "    sim_score = semantic_similarity(original, simplified)\n",
        "    comp_penalty = compression_penalty(original, simplified)\n",
        "    return sim_score * (1 - comp_penalty)\n",
        "\n",
        "# --- Compute Scores ---\n",
        "results = []\n",
        "print(\"Processing texts...\")\n",
        "for index, row in df.iterrows():\n",
        "    original_text = str(row[\"Article\"])\n",
        "    if not original_text.strip():\n",
        "        print(f\"Warning: Skipping row {index+1} due to empty 'Article'.\")\n",
        "        continue\n",
        "\n",
        "    for llm in llm_names:\n",
        "        simplified_text = str(row[llm])\n",
        "        # print(f\"Processing Article {index+1}, LLM: {llm}\") # Optional: for detailed progress\n",
        "\n",
        "        if not simplified_text.strip():\n",
        "             print(f\"Warning: Empty simplified text for Article {index+1}, LLM: {llm}. Assigning default scores.\")\n",
        "             readability_metrics = {\n",
        "                 \"Flesch-Kincaid Reading Ease\": 0,\n",
        "                 \"Flesch-Kincaid Grade Level\": 20,\n",
        "                 \"Gunning Fog Score\": 20,\n",
        "                 \"Coleman-Liau Index\": 20 # Using Coleman-Liau\n",
        "             }\n",
        "             bmps_score = 0.0\n",
        "        else:\n",
        "            readability_metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified_text),\n",
        "                \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified_text),\n",
        "                \"Gunning Fog Score\": textstat.gunning_fog(simplified_text),\n",
        "                \"Coleman-Liau Index\": textstat.coleman_liau_index(simplified_text) # Using Coleman-Liau\n",
        "            }\n",
        "            bmps_score = balanced_meaning_preservation(original_text, simplified_text)\n",
        "\n",
        "        results.append({\n",
        "            \"Article ID\": index + 1, \"LLM\": llm, **readability_metrics, \"BMPS\": bmps_score\n",
        "        })\n",
        "print(\"Finished processing texts.\")\n",
        "\n",
        "# --- Process Results ---\n",
        "if not results:\n",
        "    print(\"Error: No results were generated.\")\n",
        "else:\n",
        "    df_results = pd.DataFrame(results)\n",
        "\n",
        "    # Define metric types\n",
        "    higher_is_better = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]\n",
        "    higher_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"Coleman-Liau Index\"] # Using Coleman-Liau\n",
        "\n",
        "    # Invert metrics\n",
        "    print(\"\\nInverting 'higher is worse' metrics...\")\n",
        "    df_inverted = df_results.copy()\n",
        "    inverted_col_names = {}\n",
        "    for col in higher_is_worse:\n",
        "        if col in df_inverted.columns and df_inverted[col].notna().any():\n",
        "            max_val = df_inverted[col].max()\n",
        "            min_val = df_inverted[col].min()\n",
        "            inverted_name = f\"{col} (Inverted)\"\n",
        "            if max_val != min_val:\n",
        "                 df_inverted[col] = max_val + min_val - df_inverted[col]\n",
        "                 inverted_col_names[col] = inverted_name\n",
        "                 df_inverted.rename(columns={col: inverted_name}, inplace=True)\n",
        "            else:\n",
        "                 inverted_name = f\"{col} (Inverted - Constant)\"\n",
        "                 inverted_col_names[col] = inverted_name\n",
        "                 df_inverted.rename(columns={col: inverted_name}, inplace=True)\n",
        "                 print(f\"Warning: Column '{col}' constant, assigned name '{inverted_name}'.\")\n",
        "\n",
        "        else:\n",
        "             print(f\"Warning: Cannot invert column '{col}'. Skipping.\")\n",
        "\n",
        "    # Update metric list for normalization\n",
        "    all_metrics_post_inversion = higher_is_better + list(inverted_col_names.values())\n",
        "\n",
        "    # Normalize metrics (Min-Max)\n",
        "    print(f\"\\nNormalizing metrics: {all_metrics_post_inversion}...\")\n",
        "    df_normalized = df_inverted.copy()\n",
        "    def normalize(df, columns):\n",
        "        for col in columns:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                min_val = df[col].min()\n",
        "                max_val = df[col].max()\n",
        "                if max_val > min_val:\n",
        "                    df[col] = (df[col] - min_val) / (max_val - min_val)\n",
        "                else:\n",
        "                    df[col] = 0.5 # Assign 0.5 if constant\n",
        "            else:\n",
        "                 print(f\"Warning: Column '{col}' not found or all NaN during normalization. Skipping.\")\n",
        "        return df\n",
        "    df_normalized = normalize(df_normalized, all_metrics_post_inversion)\n",
        "\n",
        "    # Aggregate scores (Mean of Normalized Scores)\n",
        "    print(\"\\nAggregating scores...\")\n",
        "    # Ensure we only try to aggregate columns that actually exist after inversion/normalization\n",
        "    numeric_cols_for_agg = [col for col in all_metrics_post_inversion if col in df_normalized.columns and pd.api.types.is_numeric_dtype(df_normalized[col])]\n",
        "\n",
        "    if not numeric_cols_for_agg:\n",
        "         print(\"Error: No valid numeric columns found for aggregation.\")\n",
        "    else:\n",
        "        df_agg = df_normalized.groupby(\"LLM\")[numeric_cols_for_agg].mean()\n",
        "        print(\"\\n--- Aggregated Normalized Scores (Input to TOPSIS) ---\")\n",
        "        print(df_agg)\n",
        "\n",
        "        # <<< --- START: TOPSIS Calculation (replacing simple averaging) --- >>>\n",
        "        print(\"\\n--- Applying TOPSIS based on aggregated normalized scores ---\")\n",
        "\n",
        "        # Define the list of metric columns to use in TOPSIS\n",
        "        # This should match the columns used in aggregation\n",
        "        metrics_for_topsis = numeric_cols_for_agg\n",
        "\n",
        "        # --- Define Weights ---\n",
        "        # Option 1: Equal weights (as per user's last snippet)\n",
        "        num_criteria = len(metrics_for_topsis)\n",
        "        weights_array = np.array([1/num_criteria] * num_criteria)\n",
        "        print(\"Using EQUAL weights for TOPSIS.\")\n",
        "\n",
        "        # Option 2: Custom weights (Recommended - Adjust these values)\n",
        "        # Ensure weights correspond to the order in 'metrics_for_topsis'\n",
        "        # Example: {'FK Ease': 0.2, 'BMPS': 0.4, 'FK Grade (Inv)': 0.15, 'Fog (Inv)': 0.15, 'Coleman (Inv)': 0.1}\n",
        "        # Create the array carefully based on the actual order in metrics_for_topsis\n",
        "        #weight_dict = {\n",
        "       #     'Flesch-Kincaid Reading Ease': 0.20,\n",
        "       #     'BMPS': 0.40,\n",
        "       #     'Flesch-Kincaid Grade Level (Inverted)': 0.15, # Adjust name if constant col name used\n",
        "       #     'Gunning Fog Score (Inverted)': 0.15,         # Adjust name if constant col name used\n",
        "       #     'Coleman-Liau Index (Inverted)': 0.10          # Adjust name if constant col name used\n",
        "       # }\n",
        "         # Ensure all metrics for TOPSIS have weights and create array in correct order\n",
        "      #  try:\n",
        "      #      weights_array = np.array([weight_dict[col] for col in metrics_for_topsis])\n",
        "      #      # Normalize weights if they don't sum to 1 (optional but good practice)\n",
        "      #      if not np.isclose(weights_array.sum(), 1.0):\n",
        "      #          print(\"Warning: Custom weights do not sum to 1. Normalizing.\")\n",
        "      #          weights_array = weights_array / weights_array.sum()\n",
        "      #      print(f\"Using CUSTOM weights for TOPSIS: {list(zip(metrics_for_topsis, weights_array))}\")\n",
        "      #  except KeyError as e:\n",
        "      #      print(f\"Error: Missing weight for metric: {e}. Check weight_dict keys and metrics_for_topsis list.\")\n",
        "      #      weights_array = None # Prevent further execution if weights are wrong\n",
        "\n",
        "\n",
        "        if weights_array is not None:\n",
        "            # Extract the decision matrix\n",
        "            # Ensure df_agg only contains the columns defined in metrics_for_topsis for this step\n",
        "            decision_matrix = df_agg[metrics_for_topsis].to_numpy()\n",
        "\n",
        "            # Define ideal best and worst (using max/min on the aggregated normalized data)\n",
        "            ideal_best = np.max(decision_matrix, axis=0)\n",
        "            ideal_worst = np.min(decision_matrix, axis=0)\n",
        "\n",
        "            # Compute weighted Euclidean distances\n",
        "            D_plus = np.sqrt(np.sum(weights_array * (decision_matrix - ideal_best)**2, axis=1))\n",
        "            D_minus = np.sqrt(np.sum(weights_array * (decision_matrix - ideal_worst)**2, axis=1))\n",
        "\n",
        "            # Compute TOPSIS score\n",
        "            sum_D = D_plus + D_minus\n",
        "            topsis_score = np.divide(D_minus, sum_D, out=np.zeros_like(D_minus, dtype=float), where=sum_D!=0)\n",
        "\n",
        "            # Add results back to df_agg\n",
        "            df_agg['TOPSIS Score'] = topsis_score\n",
        "            df_agg['TOPSIS Rank'] = df_agg['TOPSIS Score'].rank(ascending=False, method=\"min\")\n",
        "\n",
        "            # Sort by TOPSIS Rank\n",
        "            df_ranked_topsis = df_agg.sort_values(by=\"TOPSIS Rank\")\n",
        "\n",
        "            # Display Final Ranked Results\n",
        "            print(\"\\n--- Final LLM Rankings (Based on TOPSIS) ---\")\n",
        "            # tools.display_dataframe_to_user(name=\"LLM Rankings (TOPSIS - Coleman-Liau)\", dataframe=df_ranked_topsis)\n",
        "            print(df_ranked_topsis)\n",
        "        else:\n",
        "            print(\"Could not perform TOPSIS due to weight definition error.\")\n",
        "\n",
        "        # <<< --- END: TOPSIS Calculation --- >>>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuX4kTfg0bMi",
        "outputId": "ed76e1b7-9d6c-4a6c-e327-c3adc460dce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing texts...\n",
            "Finished processing texts.\n",
            "\n",
            "Inverting 'higher is worse' metrics...\n",
            "\n",
            "Normalizing metrics: ['Flesch-Kincaid Reading Ease', 'BMPS', 'Flesch-Kincaid Grade Level (Inverted)', 'Gunning Fog Score (Inverted)', 'Coleman-Liau Index (Inverted)']...\n",
            "\n",
            "Aggregating scores...\n",
            "\n",
            "--- Aggregated Normalized Scores (Input to TOPSIS) ---\n",
            "            Flesch-Kincaid Reading Ease      BMPS  \\\n",
            "LLM                                                 \n",
            "GPT-4o                         0.436820  0.544060   \n",
            "Gemini 2.0                     0.721196  0.467568   \n",
            "Llama 3                        0.449057  0.423807   \n",
            "\n",
            "            Flesch-Kincaid Grade Level (Inverted)  \\\n",
            "LLM                                                 \n",
            "GPT-4o                                   0.398684   \n",
            "Gemini 2.0                               0.664474   \n",
            "Llama 3                                  0.405263   \n",
            "\n",
            "            Gunning Fog Score (Inverted)  Coleman-Liau Index (Inverted)  \n",
            "LLM                                                                      \n",
            "GPT-4o                          0.582353                       0.453666  \n",
            "Gemini 2.0                      0.838573                       0.639957  \n",
            "Llama 3                         0.554098                       0.388842  \n",
            "\n",
            "--- Applying TOPSIS based on aggregated normalized scores ---\n",
            "Using EQUAL weights for TOPSIS.\n",
            "\n",
            "--- Final LLM Rankings (Based on TOPSIS) ---\n",
            "            Flesch-Kincaid Reading Ease      BMPS  \\\n",
            "LLM                                                 \n",
            "Gemini 2.0                     0.721196  0.467568   \n",
            "GPT-4o                         0.436820  0.544060   \n",
            "Llama 3                        0.449057  0.423807   \n",
            "\n",
            "            Flesch-Kincaid Grade Level (Inverted)  \\\n",
            "LLM                                                 \n",
            "Gemini 2.0                               0.664474   \n",
            "GPT-4o                                   0.398684   \n",
            "Llama 3                                  0.405263   \n",
            "\n",
            "            Gunning Fog Score (Inverted)  Coleman-Liau Index (Inverted)  \\\n",
            "LLM                                                                       \n",
            "Gemini 2.0                      0.838573                       0.639957   \n",
            "GPT-4o                          0.582353                       0.453666   \n",
            "Llama 3                         0.554098                       0.388842   \n",
            "\n",
            "            TOPSIS Score  TOPSIS Rank  \n",
            "LLM                                    \n",
            "Gemini 2.0      0.876993          1.0  \n",
            "GPT-4o          0.217510          2.0  \n",
            "Llama 3         0.024751          3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import ace_tools as tools\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import textstat # Make sure textstat is imported\n",
        "\n",
        "# Load CSV\n",
        "# Using the path provided by the user previously\n",
        "file_path = \"/content/dyslexia_prompt.csv\"\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "\n",
        "# Handle potential missing values by filling with an empty string\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Extract LLM names (all columns except \"Article\")\n",
        "llm_names = df.columns[1:]  # e.g., \"GPT-4o\", \"Llama 3\", \"Gemini\"\n",
        "\n",
        "# Load sentence embedding model for BMPS calculation\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Semantic similarity function\n",
        "def semantic_similarity(text1, text2):\n",
        "    text1 = str(text1)\n",
        "    text2 = str(text2)\n",
        "    if not text1 or not text2: return 0.0\n",
        "    emb1 = model.encode(text1)\n",
        "    emb2 = model.encode(text2)\n",
        "    if np.all(emb1==0) or np.all(emb2==0): return 0.0\n",
        "    similarity = 1 - cosine(emb1, emb2)\n",
        "    return similarity if not np.isnan(similarity) else 1.0\n",
        "\n",
        "# Compression penalty function\n",
        "def compression_penalty(original, simplified):\n",
        "    original = str(original)\n",
        "    simplified = str(simplified)\n",
        "    orig_len = len(original.split())\n",
        "    simp_len = len(simplified.split())\n",
        "    if orig_len == 0: return 1.0\n",
        "    return max(0, 1 - (simp_len / orig_len))\n",
        "\n",
        "# Balanced Meaning Preservation Score (BMPS)\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    original = str(original)\n",
        "    simplified = str(simplified)\n",
        "    if not original or not simplified: return 0.0\n",
        "    sim_score = semantic_similarity(original, simplified)\n",
        "    comp_penalty = compression_penalty(original, simplified)\n",
        "    return sim_score * (1 - comp_penalty)\n",
        "\n",
        "# Compute readability and BMPS scores for each LLM\n",
        "results = []\n",
        "\n",
        "# --- Process Texts ---\n",
        "print(\"Processing texts...\")\n",
        "for index, row in df.iterrows():\n",
        "    original_text = str(row[\"Article\"])\n",
        "\n",
        "    if not original_text.strip():\n",
        "        print(f\"Warning: Skipping row {index+1} due to empty 'Article'.\")\n",
        "        continue\n",
        "\n",
        "    for llm in llm_names:\n",
        "        simplified_text = str(row[llm])\n",
        "        print(f\"Processing Article {index+1}, LLM: {llm}\")\n",
        "\n",
        "        if not simplified_text.strip():\n",
        "             print(f\"Warning: Empty simplified text for Article {index+1}, LLM: {llm}. Assigning default low/high scores.\")\n",
        "             readability_metrics = {\n",
        "                 \"Flesch-Kincaid Reading Ease\": 0, # Low ease\n",
        "                 \"Flesch-Kincaid Grade Level\": 20, # High grade level\n",
        "                 \"Gunning Fog Score\": 20, # High fog\n",
        "                 # Assign default high score for Coleman-Liau\n",
        "                 \"Coleman-Liau Index\": 20\n",
        "             }\n",
        "             bmps_score = 0.0\n",
        "        else:\n",
        "            # Compute readability metrics\n",
        "            readability_metrics = {\n",
        "                \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified_text),\n",
        "                \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified_text),\n",
        "                \"Gunning Fog Score\": textstat.gunning_fog(simplified_text),\n",
        "                # --- Changed ARI to Coleman-Liau Index ---\n",
        "                \"Coleman-Liau Index\": textstat.coleman_liau_index(simplified_text)\n",
        "            }\n",
        "\n",
        "            # Compute BMPS\n",
        "            bmps_score = balanced_meaning_preservation(original_text, simplified_text)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"Article ID\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **readability_metrics,\n",
        "            \"BMPS\": bmps_score\n",
        "        })\n",
        "\n",
        "print(\"Finished processing texts.\")\n",
        "\n",
        "# --- Process Results ---\n",
        "if not results:\n",
        "    print(\"Error: No results were generated. Check input data and processing steps.\")\n",
        "else:\n",
        "    # Convert results to DataFrame\n",
        "    df_results = pd.DataFrame(results)\n",
        "    print(\"\\n--- Raw Scores DataFrame ---\")\n",
        "    print(df_results.head())\n",
        "\n",
        "    # Define metric types\n",
        "    higher_is_better = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]\n",
        "    # --- Updated higher_is_worse list to include Coleman-Liau ---\n",
        "    higher_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"Coleman-Liau Index\"]\n",
        "\n",
        "    # --- Invert metrics where higher score means worse performance ---\n",
        "    print(\"\\nInverting 'higher is worse' metrics...\")\n",
        "    df_inverted = df_results.copy()\n",
        "    inverted_col_names = {} # To track original -> inverted names\n",
        "    for col in higher_is_worse:\n",
        "        if col in df_inverted.columns and df_inverted[col].notna().any():\n",
        "            max_val = df_inverted[col].max()\n",
        "            min_val = df_inverted[col].min()\n",
        "            inverted_name = f\"{col} (Inverted)\"\n",
        "            if max_val != min_val:\n",
        "                 df_inverted[col] = max_val + min_val - df_inverted[col]\n",
        "                 # Track rename for later list update\n",
        "                 inverted_col_names[col] = inverted_name\n",
        "                 df_inverted.rename(columns={col: inverted_name}, inplace=True)\n",
        "            else:\n",
        "                 print(f\"Warning: Cannot invert column '{col}' as all values are the same ({min_val}). Keeping original values.\")\n",
        "                 inverted_name = f\"{col} (Inverted - Constant)\"\n",
        "                 # Track rename for later list update\n",
        "                 inverted_col_names[col] = inverted_name\n",
        "                 df_inverted.rename(columns={col: inverted_name}, inplace=True)\n",
        "        else:\n",
        "             print(f\"Warning: Column '{col}' not found or contains only NaNs. Skipping inversion.\")\n",
        "\n",
        "    print(\"\\n--- Scores DataFrame After Inversion ---\")\n",
        "    print(df_inverted.head())\n",
        "\n",
        "    # Update metric list for normalization using the new inverted names\n",
        "    all_metrics_for_norm = higher_is_better + list(inverted_col_names.values())\n",
        "\n",
        "    # --- Normalize metrics using Min-Max scaling ---\n",
        "    print(f\"\\nNormalizing metrics: {all_metrics_for_norm}...\")\n",
        "    df_normalized = df_inverted.copy()\n",
        "\n",
        "    def normalize(df, columns):\n",
        "        for col in columns:\n",
        "            if col in df.columns and df[col].notna().any():\n",
        "                min_val = df[col].min()\n",
        "                max_val = df[col].max()\n",
        "                if max_val > min_val:\n",
        "                    df[col] = (df[col] - min_val) / (max_val - min_val)\n",
        "                else:\n",
        "                    df[col] = 0.5\n",
        "                    print(f\"Warning: Column '{col}' has constant values after inversion. Normalized to 0.5.\")\n",
        "            else:\n",
        "                print(f\"Warning: Column '{col}' not found or is all NaN during normalization. Skipping.\")\n",
        "        return df\n",
        "\n",
        "    # Apply normalization\n",
        "    df_normalized = normalize(df_normalized, all_metrics_for_norm)\n",
        "\n",
        "    print(\"\\n--- Scores DataFrame After Normalization ---\")\n",
        "    print(df_normalized.head())\n",
        "\n",
        "\n",
        "    # --- Aggregate scores for each LLM ---\n",
        "    print(\"\\nAggregating scores...\")\n",
        "    numeric_cols_for_agg = [col for col in all_metrics_for_norm if col in df_normalized.columns and pd.api.types.is_numeric_dtype(df_normalized[col])]\n",
        "\n",
        "    if not numeric_cols_for_agg:\n",
        "         print(\"Error: No valid numeric columns found for aggregation.\")\n",
        "    else:\n",
        "        df_agg = df_normalized.groupby(\"LLM\")[numeric_cols_for_agg].mean()\n",
        "\n",
        "        print(\"\\n--- Aggregated Normalized Scores ---\")\n",
        "        print(df_agg)\n",
        "\n",
        "        # --- Compute a total score and Rank ---\n",
        "        print(\"\\nCalculating Total Score and Rank...\")\n",
        "        df_agg[\"Total Score\"] = df_agg.mean(axis=1)\n",
        "        df_agg[\"Rank\"] = df_agg[\"Total Score\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "        # Sort by rank\n",
        "        df_ranked = df_agg.sort_values(by=\"Rank\")\n",
        "\n",
        "        # --- Display Final Ranked Results ---\n",
        "        print(\"\\n--- Final LLM Rankings (Based on Averaged Normalized Scores using Coleman-Liau) ---\")\n",
        "        # tools.display_dataframe_to_user(name=\"LLM Rankings (Coleman-Liau)\", dataframe=df_ranked)\n",
        "        print(df_ranked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q18m0AFzgAG",
        "outputId": "8e5594ff-cf75-451c-b216-164429f37e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing texts...\n",
            "Processing Article 1, LLM: GPT-4o\n",
            "Processing Article 1, LLM: Llama 3\n",
            "Processing Article 1, LLM: Gemini 2.0\n",
            "Finished processing texts.\n",
            "\n",
            "--- Raw Scores DataFrame ---\n",
            "   Article ID         LLM  Flesch-Kincaid Reading Ease  \\\n",
            "0           1      GPT-4o                        50.63   \n",
            "1           1     Llama 3                        42.07   \n",
            "2           1  Gemini 2.0                        51.14   \n",
            "\n",
            "   Flesch-Kincaid Grade Level  Gunning Fog Score  Coleman-Liau Index      BMPS  \n",
            "0                         9.2              10.36               11.70  0.261720  \n",
            "1                        10.4              11.41               14.43  0.115761  \n",
            "2                         9.0              11.54               11.70  0.260710  \n",
            "\n",
            "Inverting 'higher is worse' metrics...\n",
            "\n",
            "--- Scores DataFrame After Inversion ---\n",
            "   Article ID         LLM  Flesch-Kincaid Reading Ease  \\\n",
            "0           1      GPT-4o                        50.63   \n",
            "1           1     Llama 3                        42.07   \n",
            "2           1  Gemini 2.0                        51.14   \n",
            "\n",
            "   Flesch-Kincaid Grade Level (Inverted)  Gunning Fog Score (Inverted)  \\\n",
            "0                                   10.2                         11.54   \n",
            "1                                    9.0                         10.49   \n",
            "2                                   10.4                         10.36   \n",
            "\n",
            "   Coleman-Liau Index (Inverted)      BMPS  \n",
            "0                          14.43  0.261720  \n",
            "1                          11.70  0.115761  \n",
            "2                          14.43  0.260710  \n",
            "\n",
            "Normalizing metrics: ['Flesch-Kincaid Reading Ease', 'BMPS', 'Flesch-Kincaid Grade Level (Inverted)', 'Gunning Fog Score (Inverted)', 'Coleman-Liau Index (Inverted)']...\n",
            "\n",
            "--- Scores DataFrame After Normalization ---\n",
            "   Article ID         LLM  Flesch-Kincaid Reading Ease  \\\n",
            "0           1      GPT-4o                     0.943771   \n",
            "1           1     Llama 3                     0.000000   \n",
            "2           1  Gemini 2.0                     1.000000   \n",
            "\n",
            "   Flesch-Kincaid Grade Level (Inverted)  Gunning Fog Score (Inverted)  \\\n",
            "0                               0.857143                      1.000000   \n",
            "1                               0.000000                      0.110169   \n",
            "2                               1.000000                      0.000000   \n",
            "\n",
            "   Coleman-Liau Index (Inverted)      BMPS  \n",
            "0                            1.0  1.000000  \n",
            "1                            0.0  0.000000  \n",
            "2                            1.0  0.993077  \n",
            "\n",
            "Aggregating scores...\n",
            "\n",
            "--- Aggregated Normalized Scores ---\n",
            "            Flesch-Kincaid Reading Ease      BMPS  \\\n",
            "LLM                                                 \n",
            "GPT-4o                         0.943771  1.000000   \n",
            "Gemini 2.0                     1.000000  0.993077   \n",
            "Llama 3                        0.000000  0.000000   \n",
            "\n",
            "            Flesch-Kincaid Grade Level (Inverted)  \\\n",
            "LLM                                                 \n",
            "GPT-4o                                   0.857143   \n",
            "Gemini 2.0                               1.000000   \n",
            "Llama 3                                  0.000000   \n",
            "\n",
            "            Gunning Fog Score (Inverted)  Coleman-Liau Index (Inverted)  \n",
            "LLM                                                                      \n",
            "GPT-4o                          1.000000                            1.0  \n",
            "Gemini 2.0                      0.000000                            1.0  \n",
            "Llama 3                         0.110169                            0.0  \n",
            "\n",
            "Calculating Total Score and Rank...\n",
            "\n",
            "--- Final LLM Rankings (Based on Averaged Normalized Scores using Coleman-Liau) ---\n",
            "            Flesch-Kincaid Reading Ease      BMPS  \\\n",
            "LLM                                                 \n",
            "GPT-4o                         0.943771  1.000000   \n",
            "Gemini 2.0                     1.000000  0.993077   \n",
            "Llama 3                        0.000000  0.000000   \n",
            "\n",
            "            Flesch-Kincaid Grade Level (Inverted)  \\\n",
            "LLM                                                 \n",
            "GPT-4o                                   0.857143   \n",
            "Gemini 2.0                               1.000000   \n",
            "Llama 3                                  0.000000   \n",
            "\n",
            "            Gunning Fog Score (Inverted)  Coleman-Liau Index (Inverted)  \\\n",
            "LLM                                                                       \n",
            "GPT-4o                          1.000000                            1.0   \n",
            "Gemini 2.0                      0.000000                            1.0   \n",
            "Llama 3                         0.110169                            0.0   \n",
            "\n",
            "            Total Score  Rank  \n",
            "LLM                            \n",
            "GPT-4o         0.960183   1.0  \n",
            "Gemini 2.0     0.798615   2.0  \n",
            "Llama 3        0.022034   3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- PREREQUISITE ---\n",
        "# Assume 'df_agg' is the DataFrame resulting from the main script:\n",
        "# It contains the MEAN of NORMALIZED scores for each LLM.\n",
        "# Crucially, columns where lower was better (like Grade Level, Fog, Coleman-Liau)\n",
        "# have already been INVERTED and RENAMED (e.g., \"Metric Name (Inverted)\").\n",
        "# All columns in df_agg intended for TOPSIS should now be \"higher is better\".\n",
        "\n",
        "# --- Example df_agg structure (replace with your actual df_agg) ---\n",
        "# This structure MUST match the output of your main processing script\n",
        "data_agg_example = {\n",
        "    'Flesch-Kincaid Reading Ease': [0.8, 0.7, 0.9], # Already normalized\n",
        "    'BMPS': [0.75, 0.85, 0.7], # Already normalized\n",
        "    'Flesch-Kincaid Grade Level (Inverted)': [0.85, 0.9, 0.8], # Inverted & normalized\n",
        "    'Gunning Fog Score (Inverted)': [0.9, 0.88, 0.92], # Inverted & normalized\n",
        "    'Coleman-Liau Index (Inverted)': [0.78, 0.82, 0.75] # Inverted & normalized\n",
        "}\n",
        "llm_names_example = ['GPT-4o', 'Llama 3', 'Gemini']\n",
        "df_agg = pd.DataFrame(data_agg_example, index=llm_names_example)\n",
        "# ----------------------------------------------------------------\n",
        "\n",
        "# --- Define the list of metric columns to use in TOPSIS ---\n",
        "# These names MUST match the columns in your actual df_agg DataFrame\n",
        "# (after inversion and normalization in the main script)\n",
        "metrics_for_topsis = [\n",
        "    \"Flesch-Kincaid Reading Ease\",\n",
        "    \"BMPS\",\n",
        "    \"Flesch-Kincaid Grade Level (Inverted)\", # Use the inverted name\n",
        "    \"Gunning Fog Score (Inverted)\",        # Use the inverted name\n",
        "    \"Coleman-Liau Index (Inverted)\"         # Use the inverted name for Coleman-Liau\n",
        "    # Add/remove names based on the exact output columns of your main script\n",
        "]\n",
        "\n",
        "# Filter df_agg to include only the metrics for TOPSIS\n",
        "df_topsis_input = df_agg[metrics_for_topsis]\n",
        "\n",
        "# Use equal weights as per the original snippet's logic.\n",
        "# You can adjust weights here if desired.\n",
        "num_criteria = len(metrics_for_topsis)\n",
        "weights = np.array([1/num_criteria] * num_criteria)\n",
        "\n",
        "# Extract the decision matrix (ensure only numeric data)\n",
        "decision_matrix = df_topsis_input.to_numpy()\n",
        "\n",
        "# --- Calculations based on the user's provided snippet logic ---\n",
        "# (Note: This is a weighted Euclidean distance approach, not standard TOPSIS with vector norm)\n",
        "\n",
        "# Define the ideal best and ideal worst solutions for each criterion.\n",
        "# Assumes all criteria in decision_matrix are benefit type (higher is better)\n",
        "ideal_best = np.max(decision_matrix, axis=0)\n",
        "ideal_worst = np.min(decision_matrix, axis=0)\n",
        "\n",
        "# Compute the weighted Euclidean distance from each alternative to the ideal best and ideal worst.\n",
        "# Ensure weights array shape aligns for broadcasting if necessary (it should be 1D here)\n",
        "D_plus = np.sqrt(np.sum(weights * (decision_matrix - ideal_best)**2, axis=1))\n",
        "D_minus = np.sqrt(np.sum(weights * (decision_matrix - ideal_worst)**2, axis=1))\n",
        "\n",
        "# Compute the TOPSIS score (relative closeness to the ideal solution)\n",
        "# Handle potential division by zero if D_plus + D_minus is zero\n",
        "sum_D = D_plus + D_minus\n",
        "topsis_score = np.divide(D_minus, sum_D, out=np.zeros_like(D_minus, dtype=float), where=sum_D!=0)\n",
        "# If sum_D is 0, means D_plus=0 and D_minus=0 (alternative is both best/worst), score becomes 0. Adjust if needed.\n",
        "\n",
        "# Add the score and rank to a copy of the input df or the original df_agg\n",
        "df_ranked_topsis = df_topsis_input.copy() # Or use df_agg.copy() if you want other columns too\n",
        "df_ranked_topsis[\"TOPSIS Score\"] = topsis_score\n",
        "\n",
        "# The alternative with the higher TOPSIS Score is considered better.\n",
        "# Using 'min' rank method to handle ties like in the main script\n",
        "df_ranked_topsis[\"TOPSIS Rank\"] = df_ranked_topsis[\"TOPSIS Score\"].rank(ascending=False, method=\"min\")\n",
        "\n",
        "# Sort the LLMs based on their TOPSIS ranking.\n",
        "df_ranked_topsis = df_ranked_topsis.sort_values(by=\"TOPSIS Rank\")\n",
        "\n",
        "print(\"--- TOPSIS-Based Ranking (Using User Snippet Logic) ---\")\n",
        "# Add other columns back if needed from the original df_agg before printing\n",
        "# For example: df_ranked_topsis = df_agg.join(df_ranked_topsis[['TOPSIS Score', 'TOPSIS Rank']]).sort_values(by=\"TOPSIS Rank\")\n",
        "print(df_ranked_topsis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6xNgWKkzyDA",
        "outputId": "ff9796ae-d676-43fb-f279-5e1c4715ec65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TOPSIS-Based Ranking (Using User Snippet Logic) ---\n",
            "         Flesch-Kincaid Reading Ease  BMPS  \\\n",
            "Gemini                           0.9  0.70   \n",
            "Llama 3                          0.7  0.85   \n",
            "GPT-4o                           0.8  0.75   \n",
            "\n",
            "         Flesch-Kincaid Grade Level (Inverted)  Gunning Fog Score (Inverted)  \\\n",
            "Gemini                                    0.80                          0.92   \n",
            "Llama 3                                   0.90                          0.88   \n",
            "GPT-4o                                    0.85                          0.90   \n",
            "\n",
            "         Coleman-Liau Index (Inverted)  TOPSIS Score  TOPSIS Rank  \n",
            "Gemini                            0.75      0.513301          1.0  \n",
            "Llama 3                           0.82      0.486699          2.0  \n",
            "GPT-4o                            0.78      0.449237          3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UpKSbh1DzlKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import ace_tools as tools\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/dyslexia_prompt.csv\"\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "# Extract LLM names (all columns except \"Article\")\n",
        "llm_names = df.columns[1:]  # \"GPT-4o\", \"Llama 3\", \"Gemini\"\n",
        "\n",
        "# Load sentence embedding model for BMPS calculation\n",
        "# The 'from_tf' parameter has been removed.\n",
        "# If you need to load a model from TensorFlow, use the SentenceTransformer.from_pretrained() method.\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Semantic similarity function\n",
        "def semantic_similarity(text1, text2):\n",
        "    emb1 = model.encode(text1)\n",
        "    emb2 = model.encode(text2)\n",
        "    return 1 - cosine(emb1, emb2)  # Cosine similarity\n",
        "\n",
        "# Compression penalty function\n",
        "def compression_penalty(original, simplified):\n",
        "    orig_len = len(original.split())\n",
        "    simp_len = len(simplified.split())\n",
        "    return max(0, 1 - (simp_len / orig_len))  # Penalize longer texts\n",
        "\n",
        "# Balanced Meaning Preservation Score (BMPS)\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    sim_score = semantic_similarity(original, simplified)\n",
        "    comp_penalty = compression_penalty(original, simplified)\n",
        "    return sim_score * (1 - comp_penalty)  # Adjusted score\n",
        "\n",
        "# Compute readability and BMPS scores for each LLM\n",
        "results = []\n",
        "\n",
        "\n",
        "#\n",
        "for index, row in df.iterrows():\n",
        "    original_text = row[\"Article\"]  # The complex version\n",
        "\n",
        "    for llm in llm_names:\n",
        "        simplified_text = row[llm]\n",
        "        print(llm)\n",
        "\n",
        "        # Compute readability metrics\n",
        "        readability_metrics = {\n",
        "            \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified_text),\n",
        "            \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified_text),\n",
        "            \"Gunning Fog Score\": textstat.gunning_fog(simplified_text),\n",
        "            \"SMOG Index\": textstat.smog_index(simplified_text)\n",
        "        }\n",
        "\n",
        "        # Compute BMPS\n",
        "        bmps_score = balanced_meaning_preservation(original_text, simplified_text)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"Article ID\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **readability_metrics,\n",
        "            \"BMPS\": bmps_score\n",
        "        })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Readability metric columns\n",
        "higher_is_better = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]  # Higher is better\n",
        "higher_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"SMOG Index\"]  # Higher is worse\n",
        "\n",
        "# Invert metrics where higher = worse (so higher is always better)\n",
        "for col in higher_is_worse:\n",
        "    max_val = df_results[col].max()\n",
        "    min_val = df_results[col].min()\n",
        "    df_results[col] = max_val + min_val - df_results[col]  # Flip scale\n",
        "\n",
        "# Normalize readability metrics using Min-Max scaling\n",
        "def normalize(df, columns):\n",
        "    for col in columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        df[col] = (df[col] - min_val) / (max_val - min_val)  # Normalize between 0 and 1\n",
        "    return df\n",
        "\n",
        "# Apply normalization\n",
        "all_metrics = higher_is_better + higher_is_worse\n",
        "df_results = normalize(df_results, all_metrics)\n",
        "\n",
        "# Aggregate scores for each LLM (mean across all articles)\n",
        "df_agg = df_results.groupby(\"LLM\")[all_metrics].mean()\n",
        "\n",
        "# Compute a total score by averaging across all metrics (including BMPS)\n",
        "df_agg[\"Total Score\"] = df_agg.mean(axis=1)\n",
        "\n",
        "# Rank the LLMs based on Total Score (higher score = better ranking)\n",
        "df_agg[\"Rank\"] = df_agg[\"Total Score\"].rank(ascending=False, method=\"dense\")\n",
        "\n",
        "# Sort by rank\n",
        "df_ranked = df_agg.sort_values(by=\"Rank\")\n",
        "\n",
        "# Display results in Jupyter Notebook\n",
        "#tools.display_dataframe_to_user(name=\"LLM Rankings with BMPS\", dataframe=df_ranked)\n",
        "print(df_ranked)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znF2rhIEoZ6J",
        "outputId": "65e2f596-4ee8-4bd9-bd44-9acf793eb633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-4o\n",
            "Llama 3\n",
            "Gemini 2.0\n",
            "            Flesch-Kincaid Reading Ease      BMPS  Flesch-Kincaid Grade Level  \\\n",
            "LLM                                                                             \n",
            "GPT-4o                         0.943771  1.000000                    0.857143   \n",
            "Gemini 2.0                     1.000000  0.993077                    1.000000   \n",
            "Llama 3                        0.000000  0.000000                    0.000000   \n",
            "\n",
            "            Gunning Fog Score  SMOG Index  Total Score  Rank  \n",
            "LLM                                                           \n",
            "GPT-4o               1.000000         NaN     0.950228   1.0  \n",
            "Gemini 2.0           0.000000         NaN     0.748269   2.0  \n",
            "Llama 3              0.110169         NaN     0.027542   3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# For demonstration, assume you already have an aggregated DataFrame \"df_agg\" with normalized metrics.\n",
        "# Here, all_metrics is a list of the metric columns.\n",
        "all_metrics = [\"Flesch-Kincaid Reading Ease\", \"Flesch-Kincaid Grade Level\",\n",
        "               \"Gunning Fog Score\", \"SMOG Index\", \"BMPS\"]\n",
        "\n",
        "# If you haven't computed df_agg yet from df_results, you might have computed something like:\n",
        "df_agg = df_results.groupby(\"LLM\")[all_metrics].mean()\n",
        "\n",
        "# Use equal weights if you don't have a data-driven reason to favor any metric.\n",
        "# You can adjust the weights if necessary.\n",
        "num_criteria = len(all_metrics)\n",
        "weights = np.array([1/num_criteria] * num_criteria)\n",
        "\n",
        "# Extract the decision matrix (each row corresponds to an alternative LLM)\n",
        "decision_matrix = df_agg[all_metrics].to_numpy()\n",
        "\n",
        "# For TOPSIS, define the ideal best and ideal worst solutions for each criterion.\n",
        "ideal_best = np.max(decision_matrix, axis=0)   # For benefit criteria, the ideal is the maximum value.\n",
        "ideal_worst = np.min(decision_matrix, axis=0)    # The worst solution is the minimum value.\n",
        "\n",
        "# Compute the weighted Euclidean distance from each alternative to the ideal best and ideal worst.\n",
        "D_plus = np.sqrt(np.sum(weights * (decision_matrix - ideal_best)**2, axis=1))\n",
        "D_minus = np.sqrt(np.sum(weights * (decision_matrix - ideal_worst)**2, axis=1))\n",
        "\n",
        "# Compute the TOPSIS score (relative closeness to the ideal solution) for each alternative.\n",
        "df_agg[\"TOPSIS Score\"] = D_minus / (D_plus + D_minus)\n",
        "\n",
        "# The alternative with the higher TOPSIS Score is considered better.\n",
        "df_agg[\"TOPSIS Rank\"] = df_agg[\"TOPSIS Score\"].rank(ascending=False, method=\"dense\")\n",
        "\n",
        "# Sort the LLMs based on their TOPSIS ranking.\n",
        "df_ranked_topsis = df_agg.sort_values(by=\"TOPSIS Rank\")\n",
        "\n",
        "print(\"TOPSIS-Based Ranking:\")\n",
        "print(df_ranked_topsis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov9lUg2tviFE",
        "outputId": "e0098a8e-827f-483f-b59a-024999f89346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOPSIS-Based Ranking:\n",
            "            Flesch-Kincaid Reading Ease  Flesch-Kincaid Grade Level  \\\n",
            "LLM                                                                   \n",
            "GPT-4o                         0.943771                    0.857143   \n",
            "Gemini 2.0                     1.000000                    1.000000   \n",
            "Llama 3                        0.000000                    0.000000   \n",
            "\n",
            "            Gunning Fog Score  SMOG Index      BMPS  TOPSIS Score  TOPSIS Rank  \n",
            "LLM                                                                             \n",
            "GPT-4o               1.000000         NaN  1.000000           NaN          NaN  \n",
            "Gemini 2.0           0.000000         NaN  0.993077           NaN          NaN  \n",
            "Llama 3              0.110169         NaN  0.000000           NaN          NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ABC\n",
        "# Readability metric columns\n",
        "higher_is_better = [\"Flesch-Kincaid Reading Ease\"]  # Higher is good\n",
        "higher_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"SMOG Index\"]  # Higher is bad\n",
        "\n",
        "df = df_results.copy()\n",
        "\n",
        "# Invert metrics where higher = worse (so higher is always better)\n",
        "for col in higher_is_worse:\n",
        "    max_val = df[col].max()\n",
        "    min_val = df[col].min()\n",
        "    df[col] = max_val + min_val - df[col]  # Flip scale\n",
        "\n",
        "# Normalize readability metrics using Min-Max scaling\n",
        "def normalize(df, columns):\n",
        "    for col in columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        df[col] = (df[col] - min_val) / (max_val - min_val)  # Normalize between 0 and 1\n",
        "    return df\n",
        "\n",
        "# Apply normalization to all metrics\n",
        "all_metrics = higher_is_better + higher_is_worse\n",
        "df = normalize(df_results, all_metrics)\n",
        "\n",
        "# Aggregate scores for each LLM (mean across all publications)\n",
        "df_agg = df.groupby(\"LLM\")[all_metrics].mean()\n",
        "\n",
        "# Compute a total score by averaging across all metrics\n",
        "df_agg[\"Total Score\"] = df_agg.mean(axis=1)\n",
        "\n",
        "# Rank the LLMs based on Total Score (higher score = better ranking)\n",
        "df_agg[\"Rank\"] = df_agg[\"Total Score\"].rank(ascending=False, method=\"dense\")\n",
        "\n",
        "# Sort by rank\n",
        "df_ranked = df_agg.sort_values(by=\"Rank\")\n",
        "\n",
        "print(df_ranked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "cellView": "form",
        "id": "6VleIuhEl2jC",
        "outputId": "b7d4f082-895c-4720-9462-c3ff881ba599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Flesch-Kincaid Reading Ease  Flesch-Kincaid Grade Level  \\\n",
            "LLM                                                                   \n",
            "Original                       0.507168                    0.438770   \n",
            "GPT-4o                         0.658461                    0.244385   \n",
            "Llama 3                        0.665883                    0.241711   \n",
            "Gemini 2.0                     0.830920                    0.136364   \n",
            "\n",
            "            Gunning Fog Score  SMOG Index  Total Score  Rank  \n",
            "LLM                                                           \n",
            "Original             0.423565    0.452116     0.455405   1.0  \n",
            "GPT-4o               0.216226    0.367725     0.371699   2.0  \n",
            "Llama 3              0.230854    0.298942     0.359347   3.0  \n",
            "Gemini 2.0           0.083575    0.310053     0.340228   4.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import ace_tools as tools\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load CSV\n",
        "file_path = \"/content/dyslexia_friendly_texts_all_20.csv\"\n",
        "df = pd.read_csv(file_path, encoding='latin-1')\n",
        "\n",
        "# Extract LLM names (all columns except \"Article\")\n",
        "llm_names = df.columns[1:]  # \"GPT-4o\", \"Llama 3\", \"Gemini\"\n",
        "\n",
        "# Load sentence embedding model for BMPS calculation\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Semantic similarity function\n",
        "def semantic_similarity(text1, text2):\n",
        "    emb1 = model.encode(text1)\n",
        "    emb2 = model.encode(text2)\n",
        "    return 1 - cosine(emb1, emb2)  # Cosine similarity\n",
        "\n",
        "# Compression penalty function\n",
        "def compression_penalty(original, simplified):\n",
        "    orig_len = len(original.split())\n",
        "    simp_len = len(simplified.split())\n",
        "    return max(0, 1 - (simp_len / orig_len))  # Penalize longer texts\n",
        "\n",
        "# Balanced Meaning Preservation Score (BMPS)\n",
        "def balanced_meaning_preservation(original, simplified):\n",
        "    sim_score = semantic_similarity(original, simplified)\n",
        "    comp_penalty = compression_penalty(original, simplified)\n",
        "    return sim_score * (1 - comp_penalty)  # Adjusted score\n",
        "\n",
        "# Compute readability and BMPS scores for each LLM\n",
        "results = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    original_text = row[\"Article\"]  # The complex version\n",
        "\n",
        "    for llm in llm_names:\n",
        "        simplified_text = row[llm]\n",
        "\n",
        "        # Compute readability metrics\n",
        "        readability_metrics = {\n",
        "            \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(simplified_text),\n",
        "            \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(simplified_text),\n",
        "            \"Gunning Fog Score\": textstat.gunning_fog(simplified_text),\n",
        "            \"SMOG Index\": textstat.smog_index(simplified_text)\n",
        "        }\n",
        "\n",
        "        # Compute BMPS\n",
        "        bmps_score = balanced_meaning_preservation(original_text, simplified_text)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            \"Article ID\": index + 1,\n",
        "            \"LLM\": llm,\n",
        "            **readability_metrics,\n",
        "            \"BMPS\": bmps_score\n",
        "        })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "# Readability metric columns\n",
        "higher_is_better = [\"Flesch-Kincaid Reading Ease\", \"BMPS\"]  # Higher is better\n",
        "higher_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"SMOG Index\"]  # Higher is worse\n",
        "\n",
        "# Invert metrics where higher = worse (so higher is always better)\n",
        "for col in higher_is_worse:\n",
        "    max_val = df_results[col].max()\n",
        "    min_val = df_results[col].min()\n",
        "    df_results[col] = max_val + min_val - df_results[col]  # Flip scale\n",
        "\n",
        "# Normalize readability metrics using Min-Max scaling\n",
        "def normalize(df, columns):\n",
        "    for col in columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        df[col] = (df[col] - min_val) / (max_val - min_val)  # Normalize between 0 and 1\n",
        "    return df\n",
        "\n",
        "# Apply normalization\n",
        "all_metrics = higher_is_better + higher_is_worse\n",
        "df_results = normalize(df_results, all_metrics)\n",
        "\n",
        "# Aggregate scores for each LLM (mean across all articles)\n",
        "df_agg = df_results.groupby(\"LLM\")[all_metrics].mean()\n",
        "\n",
        "# Compute a total score by averaging across all metrics (including BMPS)\n",
        "df_agg[\"Total Score\"] = df_agg.mean(axis=1)\n",
        "\n",
        "# Rank the LLMs based on Total Score (higher score = better ranking)\n",
        "df_agg[\"Rank\"] = df_agg[\"Total Score\"].rank(ascending=False, method=\"dense\")\n",
        "\n",
        "# Sort by rank\n",
        "df_ranked = df_agg.sort_values(by=\"Rank\")\n",
        "\n",
        "# Display results in Jupyter Notebook\n",
        "#tools.display_dataframe_to_user(name=\"LLM Rankings with BMPS\", dataframe=df_ranked)\n",
        "print(df_ranked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2N68htfgH1d",
        "outputId": "2cb7127a-57ca-4df1-b67f-deccf0a3f899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Flesch-Kincaid Reading Ease      BMPS  Flesch-Kincaid Grade Level  \\\n",
            "LLM                                                                             \n",
            "GPT-4o                         0.943771  1.000000                    0.857143   \n",
            "Gemini 2.0                     1.000000  0.998215                    1.000000   \n",
            "Llama 3                        0.000000  0.000000                    0.000000   \n",
            "\n",
            "            Gunning Fog Score  SMOG Index  Total Score  Rank  \n",
            "LLM                                                           \n",
            "GPT-4o               1.000000         NaN     0.950228   1.0  \n",
            "Gemini 2.0           0.000000         NaN     0.749554   2.0  \n",
            "Llama 3              0.110169         NaN     0.027542   3.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the CSV file since execution state was reset\n",
        "import pandas as pd\n",
        "import textstat\n",
        "\n",
        "# File path\n",
        "file_path = \"/content/dyslexia_friendly_texts_all_20.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract only the original articles\n",
        "df_originals = df[['Article']].copy()\n",
        "\n",
        "# Function to calculate readability metrics\n",
        "def calculate_readability_metrics(text):x\n",
        "    return {\n",
        "        \"Flesch-Kincaid Reading Ease\": textstat.flesch_reading_ease(text),\n",
        "        \"Flesch-Kincaid Grade Level\": textstat.flesch_kincaid_grade(text),\n",
        "        \"Gunning Fog Score\": textstat.gunning_fog(text),\n",
        "        \"SMOG Index\": textstat.smog_index(text)\n",
        "    }\n",
        "\n",
        "# Compute readability metrics for each original article\n",
        "readability_results = []\n",
        "for index, row in df_originals.iterrows():\n",
        "    metrics = calculate_readability_metrics(row['Article'])\n",
        "    metrics['Article Index'] = index + 1  # Keep track of the article number\n",
        "    readability_results.append(metrics)\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "df_readability = pd.DataFrame(readability_results)\n",
        "\n",
        "# Normalize the readability scores for fair comparison\n",
        "def normalize(df, columns):\n",
        "    for col in columns:\n",
        "        min_val = df[col].min()\n",
        "        max_val = df[col].max()\n",
        "        df[col] = (df[col] - min_val) / (max_val - min_val)  # Normalize between 0 and 1\n",
        "    return df\n",
        "\n",
        "# Invert metrics where lower = better (so all higher scores mean \"easier for dyslexic readers\")\n",
        "harder_is_worse = [\"Flesch-Kincaid Grade Level\", \"Gunning Fog Score\", \"SMOG Index\"]\n",
        "for col in harder_is_worse:\n",
        "    max_val = df_readability[col].max()\n",
        "    min_val = df_readability[col].min()\n",
        "    df_readability[col] = max_val + min_val - df_readability[col]\n",
        "\n",
        "# Normalize all metrics\n",
        "all_metrics = [\"Flesch-Kincaid Reading Ease\"] + harder_is_worse\n",
        "df_readability = normalize(df_readability, all_metrics)\n",
        "\n",
        "# Compute an overall dyslexia-friendliness score (higher = easier to read)\n",
        "df_readability[\"Dyslexia-Friendliness Score\"] = df_readability[all_metrics].mean(axis=1)\n",
        "\n",
        "# Rank articles based on their readability (higher score = more dyslexia-friendly)\n",
        "df_readability[\"Rank\"] = df_readability[\"Dyslexia-Friendliness Score\"].rank(ascending=False, method=\"dense\")\n",
        "\n",
        "# Sort articles by rank\n",
        "df_readability_sorted = df_readability.sort_values(by=\"Rank\")\n",
        "\n",
        "print(df_readability_sorted)\n",
        "# Display results\n",
        "#import ace_tools as tools\n",
        "#tools.display_dataframe_to_user(name=\"Original Articles Readability Analysis\", dataframe=df_readability_sorted)\n"
      ],
      "metadata": {
        "id": "Xij1PFqsZm4s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "94ce3454-a5f8-4b37-a615-ec40642bbb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-9-bdfe26965f3d>, line 16)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-bdfe26965f3d>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    return {\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat\n",
        "\n",
        "def count_words(text):\n",
        "    return textstat.lexicon_count(text)\n",
        "\n",
        "df['Word Count'] = df['Article'].apply(count_words)\n",
        "\n",
        "# Create 'Article Index' column in df to match df_readability_sorted\n",
        "df['Article Index'] = df.index + 1\n",
        "\n",
        "df_readability_sorted = pd.merge(df_readability_sorted, df[['Article Index', 'Word Count']], on='Article Index', how='left')\n",
        "\n",
        "print(df_readability_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "P-jqQkZ5iu5Y",
        "outputId": "d7c61a78-6ba9-41a9-abed-51436124ff15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_readability_sorted' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a2169c78345a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Article Index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf_readability_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_readability_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Article Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word Count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Article Index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_readability_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_readability_sorted' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2gkqOHrnrv2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}